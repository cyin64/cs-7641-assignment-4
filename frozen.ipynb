{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import hiive.mdptoolbox as mdptoolbox\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import pygame\n",
    "from algorithms.rl import RL\n",
    "from algorithms.planner import Planner\n",
    "from examples.test_env import TestEnv\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# suppress pandas warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    b'S': 'b',\n",
    "    b'F': 'w',\n",
    "    b'H': 'k',\n",
    "    b'G': 'g'\n",
    "}\n",
    "\n",
    "directions = {\n",
    "            0: '←',\n",
    "            1: '↓',\n",
    "            2: '→',\n",
    "            3: '↑'\n",
    "}\n",
    "\n",
    "def plot_lake(env, policy=None, title='Frozen Lake'):\n",
    "    squares = env.nrow\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, xlim=(-.01, squares+0.01), ylim=(-.01, squares+0.01))\n",
    "    plt.title(title, fontsize=16, weight='bold', y=1.01)\n",
    "    for i in range(squares):\n",
    "        for j in range(squares):\n",
    "            y = squares - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1, linewidth=0.5, edgecolor='k')\n",
    "            p.set_facecolor(colors[env.desc[i,j]])\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            if policy is not None:\n",
    "                if (i, j)!=(7,7):\n",
    "                    text = ax.text(x+0.5, y+0.5, directions[policy[i, j]],\n",
    "                               horizontalalignment='center', size=20, verticalalignment='center',\n",
    "                               color='k')\n",
    "            \n",
    "    plt.axis('off')\n",
    "    # plt.savefig('./frozen/' + title + '.png', dpi=400)\n",
    "    \n",
    "# plot_lake(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 8x8\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1', render_mode=None)\n",
    "\n",
    "plot_lake(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of iterations\n",
    "\n",
    "def number_iteration(V_track, max_iteration):\n",
    "    iteration =1\n",
    "    for i in range(max_iteration):\n",
    "        if not np.all((V_track[i] == 0)):\n",
    "            iteration += 1\n",
    "    return iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.3, 0.6, 0.9, 0.99]\n",
    "epsilons = [1e-2, 1e-5, 1e-8, 1e-12]\n",
    "\n",
    "y = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    for epsilon in epsilons:   \n",
    "\n",
    "        # output success percentage of VI \n",
    "        V, V_track, pi = Planner(env.P).value_iteration(gamma=gamma, n_iters=10000, theta=epsilon)\n",
    "        test_scores = TestEnv.test_env(env=env, render=True, n_iters=100, user_input=False, pi=pi)\n",
    "        print (gamma, epsilon, test_scores.mean())\n",
    "\n",
    "        # output success percentage of PI \n",
    "        V, V_track, pi = Planner(env.P).policy_iteration(gamma=gamma, n_iters=10000, theta=epsilon)\n",
    "        test_scores = TestEnv.test_env(env=env, render=True, n_iters=50, user_input=False, pi=pi)\n",
    "        print (gamma, epsilon, test_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.3, 0.6, 0.9, 0.99]\n",
    "epsilons = [1e-2, 1e-5, 1e-8, 1e-12]\n",
    "\n",
    "for gamma in gammas:\n",
    "    for epsilon in epsilons: \n",
    "\n",
    "        # run the VI algorithm\n",
    "        V, V_track, pi = Planner(env.P).value_iteration(gamma=gamma, n_iters=10000, theta=epsilon)\n",
    "\n",
    "        # output the iteration of VI \n",
    "        iteration = number_iteration(V_track, 10000)\n",
    "        print(gamma, epsilon, iteration)\n",
    "\n",
    "        # run the PI algorithm\n",
    "        V, V_track, pi = Planner(env.P).policy_iteration(gamma=gamma, n_iters=10000, theta=epsilon)\n",
    "\n",
    "        # output the iteration of PI \n",
    "        iteration = number_iteration(V_track, 10000)\n",
    "        print(gamma, epsilon, iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.3, 0.6, 0.9, 0.99]\n",
    "epsilons = [1e-2, 1e-5, 1e-8, 1e-12]\n",
    "\n",
    "for gamma in gammas:\n",
    "    for epsilon in epsilons: \n",
    "\n",
    "        # run the VI algorithm\n",
    "        V, V_track, pi = Planner(env.P).value_iteration(gamma=gamma, n_iters=100000, theta=epsilon)\n",
    "        new_pi = list(map(lambda x: pi(x), range(64)))\n",
    "        new_pi = np.around(np.array(new_pi).reshape((8, 8)), 2)\n",
    "\n",
    "        # plot the policy\n",
    "        title='Frozen Lake VI Optimal Policy ({}, {})'.format(gamma, epsilon)\n",
    "        plot_lake(env, new_pi, title)\n",
    "\n",
    "        # run the PI algorithm\n",
    "        V, V_track, pi = Planner(env.P).value_iteration(gamma=gamma, n_iters=100000, theta=epsilon)\n",
    "        new_pi = list(map(lambda x: pi(x), range(64)))\n",
    "        new_pi = np.around(np.array(new_pi).reshape((8, 8)), 2)\n",
    "\n",
    "        # plot the policy\n",
    "        title='Frozen Lake PI Optimal Policy ({}, {})'.format(gamma, epsilon)\n",
    "        plot_lake(env, new_pi, title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.8, 0.9, 0.99]\n",
    "alphas   = [0.01, 0.1, 0.2]\n",
    "# alpha_decays = [0.9, 0.999]\n",
    "# epsilon_decays = [0.9, 0.999]\n",
    "iterations = [500000, 1000000, 5000000]\n",
    "\n",
    "for gamma in gammas:\n",
    "    for alpha in alphas:\n",
    "        for iteration in iterations:\n",
    "            \n",
    "            Q, V, pi, Q_track, pi_track = RL(env).q_learning(gamma=gamma, init_alpha=alpha, n_episodes=iteration)\n",
    "\n",
    "            print(gamma, alpha, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gammas   = [0.8, 0.9, 0.99]\n",
    "alphas   = [0.01, 0.1, 0.2]\n",
    "alpha_decays = [0.9, 0.999]\n",
    "epsilon_decays = [0.9, 0.999]\n",
    "iterations = [500000, 1000000, 5000000]\n",
    "\n",
    "for gamma in gammas:\n",
    "    for alpha in alphas:\n",
    "        for iteration in iterations:\n",
    "            \n",
    "            Q, V, pi, Q_track, pi_track = RL(env).q_learning(gamma=gamma, init_alpha=alpha, n_episodes=iteration)\n",
    "            new_pi = list(map(lambda x: pi(x), range(64)))\n",
    "            new_pi = np.around(np.array(new_pi).reshape((8, 8)), 2)\n",
    "\n",
    "            # plot the policy\n",
    "            title='FL Q-Learning ({}, {}, {})'.format(gamma, alpha, iteration)\n",
    "            plot_lake(env, new_pi, title)\n",
    "\n",
    "            # calculate the success percentage\n",
    "            test_scores = TestEnv.test_env(env=env, render=True, n_iters=100, user_input=False, pi=pi)\n",
    "            print(gamma, alpha, iteration, test_scores.mean())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
